# Paths
monodepth_cfg_path: "src_betr/configs/da3mono-large.yaml"
monodepth_checkpoint_path: "src_betr/checkpoints/DA3Mono_Large.safetensors"
metricdepth_cfg_path: "src_betr/configs/da3metric-large.yaml"
metricdepth_checkpoint_path: "src_betr/checkpoints/DA3Metric_Large.safetensors"
dinov3_checkpoint_path: "src_betr/checkpoints/dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth"
wds_root: "datasets/betr_wds"
json_root: "datasets/L2V_2d_ordered"
data_root: "datasets/"

# System
device: "cuda"
model_name: "betr_model_corners_v2" # Model with Z-Score and Increasing lambda of fine loss
seed: 42
  
# Data
data:
  da3_image_size: 448
  dino_image_size: 512
  target_quality: "Good"
  min_area_object: 1024 # 32x32
epoch_length: 50000

aug:
  box_jitter_sigma: 5e-3
  feature_noise_sigma: 1e-2
  feature_dropout: 0.1

# Model Dimensions
feature_generator_dim: 512  # Decreasing Fine loss from 5.0 -> 2.0, feature augmentation, offset heads initializaiton
d_model: 256                # Transformer model dimension
skip_channels: 256        # Channels for skip connection after feature generator

# Model Hyperparameters
dropout: 0.1
activation: "relu"
feature_mode: True  # Whether to use feature generator a.k.a. training mode
num_outputs: 27
# Number of outputs for prediction head (center coords: 2, center depth: 1, bb8 offsets: 16, bb8 depth offsets: 8)

# Prediction Heads Config
prediction_heads: [
  "corner heatmaps", "corner depths"
]
soft_argmax:
  beta: 100.0
  is_sigmoid: False

# Transformer Encoder Config
encoder:
  nhead: 8
  dim_feedforward: 1024
  num_layers: 6

# Transformer Decoder Config
decoder:
  d_model: 256
  nhead: 8
  dim_feedforward: 1024
  num_layers: 6

# Positional Encoding Config
position_encoding:
  temperature: 10000
  normalize: False
  scale: null

# Box Embedding Config
box_embedding:
  temperature: 10000
  scale: null

# Loss configs
loss_weights:
  lambda_: 2.0 # Weights for fine loss
  sigma_: 1.2 # heatmap std
loss_v2_weights:
  center: 1.0
  offset: 1.0
  depth: 1.0
  d_offset: 1.0
# Hyperparameters
num_epochs: 100
batch_size: 16
learning_rate: 1e-4
weight_decay: 1e-4
num_workers: 1

# Validation and Logging
val_interval: 1
save_interval: 25