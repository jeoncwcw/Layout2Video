# Paths
monodepth_cfg_path: "src_betr/configs/da3mono-large.yaml"
monodepth_checkpoint_path: "src_betr/checkpoints/DA3Mono_Large.safetensors"
metricdepth_cfg_path: "src_betr/configs/da3metric-large.yaml"
metricdepth_checkpoint_path: "src_betr/checkpoints/DA3Metric_Large.safetensors"
dinov3_checkpoint_path: "src_betr/checkpoints/dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth"
wds_root: "datasets/betr_wds"
json_root: "datasets/L2V_2d_ordered"
data_root: "datasets/"

# System
device: "cuda"
model_name: "betr_model_corners_v4" # Model with Z-Score and Increasing lambda of fine loss
seed: 42
  
# Data
data:
  da3_image_size: 448
  dino_image_size: 512
  target_quality: "Good"
  min_area_object: 1024 # 32x32
epoch_length: 50000

aug:
  box_jitter_sigma: 1e-3
  feature_noise_sigma: 1e-2
  feature_dropout: 0.1
  flip_prob: 0.2
  rot_range: 5.0

# Model Dimensions
feature_generator_dim: 512  # Decreasing Fine loss from 5.0 -> 2.0, feature augmentation, offset heads initializaiton
d_model: 512                # Transformer model dimension
skip_channels: 256        # Channels for skip connection after feature generator

# Model Hyperparameters
dropout: 0.1
activation: "relu"
feature_mode: True  # Whether to use feature generator a.k.a. training mode

# Prediction Heads Config
prediction_heads: [
  "corner heatmaps", "corner depths"
]
soft_argmax:
  beta: 100.0
  is_sigmoid: False

# Transformer Encoder Config
encoder:
  nhead: 8
  dim_feedforward: 1024
  num_layers: 6

# Transformer Decoder Config
decoder:
  nhead: 8
  dim_feedforward: 1024
  num_layers: 6

# Positional Encoding Config
position_encoding:
  temperature: 10000
  normalize: False
  scale: null

# Box Embedding Config
box_embedding:
  temperature: 10000
  scale: null

# Loss configs
loss_weights:
  lambda_: 5.0 # Weights for fine loss
  depth: 1.0 # heatmap std
loss_v2_weights:
  center: 1.0
  offset: 1.0
  depth: 1.0
  d_offset: 1.0
# Hyperparameters
# maintain grad_accum_steps * batch_size * world_size = 48
num_epochs: 100
warmup_epochs: 5
grad_accum_steps: 2
batch_size: 8
learning_rate: 1e-4
weight_decay: 1e-4
num_workers: 2

# Validation and Logging
val_interval: 4
save_interval: 25